---
title: "p8105_hw5_cx2227"
author: "Chuyue Xiang"
date: "11/7/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rvest)
```


## Problem 1

```{r}
library(tidyverse)

set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
```

```{r}
replace_missing_values = function(vec) {
  if(is.numeric(vec))
  {vec = replace(vec, is.na(vec), round(mean(vec, na.rm = TRUE), digits = 1))}
  else if (is.character(vec)) {
    vec = replace(vec, is.na(vec), "virginica")}
  }
```

```{r}
iris_with_missing %>%
  map_df(~replace_missing_values(.x)) %>%
  knitr::kable()
```

## Problem 2

```{r}
tibble(list.files(path = "./data")) %>% 
  rename(arm_id = `list.files(path = "./data")`) %>% 
  mutate(list1 = "./data/", list2 = str_c(list1, arm_id) ,data = map(list2, read.csv)) %>% 
  unnest() %>% 
  
  pivot_longer(
    cols = starts_with("week"),
    names_to = "week",
    names_prefix = "week_",
    values_to = "data"
    ) %>% 
  subset.data.frame(select = c("arm_id","week","data")) %>% 
  
  ggplot(aes(x = week, y = data, group = arm_id, color = arm_id)) +
  geom_path() +
  labs(
    title = "Observations of two arms of participants over 8 weeks",
    x = "Week", 
    y = "Value"
    ) 
```

 - From the plot, we can see that generally, experiment group had a larger value than the control group. And the experiment group experienced a significant growth during the 8 weeks when control group stayed mostly the same level as they started.


## Problem 3

```{r}
sim_regression = function(n = 30, beta0 = 2, beta1 = 0) {
  
  sim_data = tibble(
    x = rnorm(n, mean = 1, sd = 1),
    y = beta0 + beta1 * x + rnorm(n, 0, sqrt(50))
  )
  
  ls_fit = lm(y ~ x, data = sim_data)
  
  tibble(
    beta1_hat = coef(ls_fit)[2],
    p_value = broom::tidy(ls_fit)[[2,5]]
  )
}

sim_regression()

sim_results_beta1_0 = 
  rerun(10000, sim_regression(30, 2, 0)) %>% 
  bind_rows() %>% 
  mutate(beta = 0)

sim_results_beta1_1 = 
  rerun(10000, sim_regression(30, 2, 1)) %>% 
  bind_rows() %>% 
  mutate(beta = 1)

sim_results_beta1_2 = 
  rerun(10000, sim_regression(30, 2, 2)) %>% 
  bind_rows() %>% 
  mutate(beta = 2)

sim_results_beta1_3 = 
  rerun(10000, sim_regression(30, 2, 3)) %>% 
  bind_rows() %>% 
  mutate(beta = 3)

sim_results_beta1_4 = 
  rerun(10000, sim_regression(30, 2, 4)) %>% 
  bind_rows() %>% 
  mutate(beta = 4)

sim_results_beta1_5 = 
  rerun(10000, sim_regression(30, 2, 5)) %>% 
  bind_rows() %>% 
  mutate(beta = 5)

sim_results_beta1_6 = 
  rerun(10000, sim_regression(30, 2, 6)) %>% 
  bind_rows() %>% 
  mutate(beta = 6)

bind_rows(sim_results_beta1_0, sim_results_beta1_1, sim_results_beta1_2, sim_results_beta1_3, sim_results_beta1_4, sim_results_beta1_5, sim_results_beta1_6) %>% 
  mutate(reject = case_when(p_value < 0.05 ~"1", TRUE ~"0")) %>% 
  
  group_by(beta) %>% 
  summarize(reject_sum = sum(as.numeric(reject)), n_obs = n()) %>% 
  mutate(proportion = reject_sum/n_obs) %>%
  
  ggplot(aes(x = beta, y = proportion)) +
  geom_bar(stat = "identity")+
  scale_x_discrete(limits=c("1","2","3","4","5","6"))+
  geom_text(aes(label=proportion))

```

 - From the bar chart, we can see that as the effect size grows, more and more false null hypothesis got rejected.
 
```{r}
reject_summary = bind_rows(sim_results_beta1_0, sim_results_beta1_1, sim_results_beta1_2, sim_results_beta1_3, sim_results_beta1_4, sim_results_beta1_5, sim_results_beta1_6) %>% 
  filter(p_value < 0.05) %>% 
  group_by(beta) %>% 
  summarize(avg_beta1 = mean(beta1_hat))

reject_summary %>%   
ggplot(aes(x = beta, y = avg_beta1)) + 
  geom_point(data = reject_summary)


bind_rows(sim_results_beta1_0, sim_results_beta1_1, sim_results_beta1_2, sim_results_beta1_3, sim_results_beta1_4, sim_results_beta1_5, sim_results_beta1_6) %>% 
  group_by(beta) %>% 
  summarize(avg_beta1 = mean(beta1_hat)) %>% 
  
  ggplot(aes(x = beta, y = avg_beta1)) + 
  geom_point()
```

 - Is the sample average of β̂ 1 across tests for which the null is rejected approximately equal to the true value of β1? Why or why not?
 - The sample average of β̂ 1 across tests for which the null is rejected is not approximately equal to the true value. Because larger the effective size, more false null hypothesis got rejected. These rejected beta have p-value less than 0.05 will not be a good estimator of the ture value.
